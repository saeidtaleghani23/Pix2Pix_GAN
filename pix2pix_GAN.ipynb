{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix GAN\n",
    "### It's a type of cGAN where a preceding image serves as a condition for image generation such as translating one image into another.\n",
    "### For example, colorizing black and white photos, transforming satellite images into map routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os \n",
    "import pathlib \n",
    "import time\n",
    "import datetime \n",
    "from matplotlib import pyplot as plt \n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"maps\"#@param [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\", \"night2day\"]\n",
    "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
    "# get_file function is used to download the dataset file and extract it\n",
    "path2zip = tf.keras.utils.get_file(fname=f\"{dataset_name}.tar.gz\", origin = _URL, extract= True)\n",
    "# pathlib.Path is module is used to handle file paths\n",
    "path2zip = pathlib.Path(path2zip)\n",
    "# 'PATH' variable represents the path to the extracted dataset.\n",
    "PATH = path2zip.parent/dataset_name\n",
    "\n",
    "#  tf.io.read_file function reads the file content as a binary string\n",
    "sample_image = tf.io.read_file(str(PATH/'train/1.jpg'))\n",
    "# tf.io.decode_jpeg decodes the binary string into a tensor representing the image\n",
    "sample_image = tf.io.decode_jpeg(sample_image)\n",
    "print(f'shape of the sample image is: {sample_image.shape}')\n",
    "plt.figure()\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "BATCH_SIZE = 1  # Depending on your GPU memory\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Function to preprocess images: resize, normalize, etc.\n",
    "def load(image_file):\n",
    "    # Read the image from file\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    # Split input and target images from the dataset\n",
    "    w = tf.shape(image)[1] // 2\n",
    "    input_image = image[:, :w, :]  # Input image\n",
    "    target_image = image[:, w:, :]  # Target image\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    target_image = tf.cast(target_image, tf.float32)\n",
    "\n",
    "    return input_image, target_image\n",
    "\n",
    "def resize(input_image, target_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width])\n",
    "    target_image = tf.image.resize(target_image, [height, width])\n",
    "    return input_image, target_image\n",
    "\n",
    "def normalize(input_image, target_image):\n",
    "    # Normalize to the range [-1, 1] as used in GANs\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    target_image = (target_image / 127.5) - 1\n",
    "    return input_image, target_image\n",
    "\n",
    "def load_image_train(image_file):\n",
    "    input_image, target_image = load(image_file)\n",
    "    input_image, target_image = resize(input_image, target_image, IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, target_image = normalize(input_image, target_image)\n",
    "    return input_image, target_image\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    input_image, target_image = load(image_file)\n",
    "    input_image, target_image = resize(input_image, target_image, IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, target_image = normalize(input_image, target_image)\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "train_path = PATH / 'train'  # Assuming the dataset has a 'train' folder\n",
    "test_path = PATH / 'val'     # Assuming the dataset has a 'val' folder\n",
    "\n",
    "def load_dataset(dataset_path, load_image_fn):\n",
    "    # Get a list of image file paths\n",
    "    dataset = tf.data.Dataset.list_files(str(dataset_path/'*.jpg'))\n",
    "    \n",
    "    # Load and preprocess the images\n",
    "    dataset = dataset.map(load_image_fn, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # Shuffle, batch, and prefetch\n",
    "    dataset = dataset.shuffle(buffer_size=400)  # Adjust buffer_size as needed\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create training and test datasets\n",
    "train_dataset = load_dataset(train_path, load_image_train)\n",
    "test_dataset = load_dataset(test_path, load_image_test)\n",
    "\n",
    "# Get a sample from the training dataset\n",
    "for input_image, target_image in train_dataset.take(1):\n",
    "    print(f\"Train Input image shape: {input_image.shape}\")\n",
    "    print(f\"Train Target image shape: {target_image.shape}\")\n",
    "\n",
    "# Similarly, for the test dataset\n",
    "for input_image, target_image in test_dataset.take(1):\n",
    "    print(f\"Test Input image shape: {input_image.shape}\")\n",
    "    print(f\"Test Target image shape: {target_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "Output_channel = 3\n",
    "def downsample (filters: int, kernel_size: int,  stride: int = 2, apply_batchnorm: bool=True):\n",
    "    \"\"\" this function applies down sampling in the Encoder part of Unet Generator model\n",
    "\n",
    "    Args:\n",
    "        filters (int): number of filters in the convolutional layer\n",
    "        kernel_size (int): size of the filters\n",
    "        apply_batchnorm (bool, optional): applying batch normalization after the convolutional layer. Defaults to True.\n",
    "    \"\"\"\n",
    "    # weight initialization\n",
    "    initializer = tf.random_normal_initializer (0., 0.02)\n",
    "    # create down sampling layer\n",
    "    downsampled_feature_map = tf.keras.Sequential()\n",
    "    downsampled_feature_map.add(tf.keras.layers.Conv2D(filters, \n",
    "                                                       kernel_size, \n",
    "                                                       strides=stride, \n",
    "                                                       padding = 'same', \n",
    "                                                       kernel_initializer = initializer, \n",
    "                                                       use_bias = False))\n",
    "    # conditionally add batch normalization\n",
    "    if apply_batchnorm:\n",
    "        downsampled_feature_map.add(tf.keras.layers.BatchNormalization(momentum= 0.8))\n",
    "    # leaky relu activation    \n",
    "    downsampled_feature_map.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    return downsampled_feature_map\n",
    "\n",
    "\n",
    "# Decoder\n",
    "def upsample(filters: int, kernel_size: int, apply_dropout : bool= False):\n",
    "    \"\"\"This is a function definition for creating an up sampling block in Encoder part of the U-Net model\n",
    "\n",
    "    Args:\n",
    "        filters (int): number of filters in the transposed convolutional layer\n",
    "        kernel_size (int):  size of the filters\n",
    "        apply_dropout (bool, optional): applying drop out regularization after the transposed convolutional layer. Defaults to False.\n",
    "    \"\"\"\n",
    "    # weight initialization\n",
    "    initializer = tf.random_normal_initializer (0., 0.02)\n",
    "    # create up sampling layer\n",
    "    upsampled_feature_map = tf.keras.Sequential() \n",
    "    upsampled_feature_map.add(tf.keras.layers.Conv2DTranspose(filters, kernel_size, strides=2, padding = 'same', kernel_initializer=initializer, use_bias= False ))\n",
    "    # add batch normalization\n",
    "    upsampled_feature_map.add(tf.keras.layers.BatchNormalization(momentum= 0.8))\n",
    "    # conditionally add dropout\n",
    "    if apply_dropout:\n",
    "        upsampled_feature_map.add(tf.keras.layers.Dropout(0.5))\n",
    "    # relu activation\n",
    "    upsampled_feature_map.add(tf.keras.layers.ReLU())\n",
    "    return upsampled_feature_map\n",
    "\n",
    "# Generator Class\n",
    "def Generator(image_shape=(256, 256, 3)):\n",
    "    input_image=tf.keras.layers.Input(shape=image_shape)\n",
    "    #C64-C128-C256-C512-C512-C512-C512-C512\n",
    "    down_stack = [\n",
    "        downsample(filters=64, kernel_size=4, apply_batchnorm=False),\n",
    "        downsample(filters=128, kernel_size=4, apply_batchnorm=True),\n",
    "        downsample(filters=256, kernel_size=4, apply_batchnorm=True),\n",
    "        downsample(filters=512, kernel_size=4, apply_batchnorm=True),\n",
    "        downsample(filters=512, kernel_size=4, apply_batchnorm=True),\n",
    "        downsample(filters=512, kernel_size=4, apply_batchnorm=True),\n",
    "        downsample(filters=512, kernel_size=4, apply_batchnorm=True),\n",
    "        downsample(filters=512, kernel_size=4, apply_batchnorm=False), # bottleneck, no batch norm \n",
    "    ]\n",
    "    # CD512-CD512-CD512-CD512-C256-C128-C64\n",
    "    up_stack = [\n",
    "        upsample(filters=512, kernel_size=4, apply_dropout=True),\n",
    "        upsample(filters=512, kernel_size=4, apply_dropout=True),\n",
    "        upsample(filters=512, kernel_size=4, apply_dropout=True),\n",
    "        upsample(filters=512, kernel_size=4, apply_dropout=False),\n",
    "        upsample(filters=256, kernel_size=4, apply_dropout=False),\n",
    "        upsample(filters=128, kernel_size=4, apply_dropout=False),\n",
    "        upsample(filters=64, kernel_size=4, apply_dropout=False),\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    x= input_image\n",
    "    skips = []\n",
    "    # Encoder\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        # keep teh output of each layer for using it as skip connection\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1]) # we remove the bottleneck while the x includes the bottleneck\n",
    "    # Decoder\n",
    "    for up, skip in zip (up_stack, skips):\n",
    "        # the first x is the bottleneck\n",
    "        x = up(x)\n",
    "        x= tf.keras.layers.Concatenate()([x, skip])\n",
    "    # activation=tanh makes sure that the output values are between -1 and 1\n",
    "    output_image= tf.keras.layers.Conv2DTranspose (filters = Output_channel, \n",
    "                                                  kernel_size = 4, \n",
    "                                                  strides = 2, \n",
    "                                                  padding= 'same',\n",
    "                                                  kernel_initializer= initializer,\n",
    "                                                  activation= 'tanh') (x)\n",
    "    generative_model = tf.keras.Model(inputs = input_image, outputs = output_image)\n",
    "    return generative_model\n",
    "gen_model = Generator()\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchGAN Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # A 70x70 PatchGAN will classify 70x70 patches of the input image as real or fake\n",
    "def Discriminator(image_shape=(256, 256, 3)):\n",
    "    \"\"\"Builds a PatchGAN-based Discriminator model for a Conditional GAN.\n",
    "    This discriminator is designed to classify whether each 70x70 patch in the input \n",
    "    image is real or generated (fake). It concatenates the input image and target image \n",
    "    and applies several convolutional layers to process this input, following the \n",
    "    architecture described in the PatchGAN model.\n",
    "\n",
    "    Args:\n",
    "       image_shape (tuple): The shape of the input image. Default is (256, 256, 3).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled Keras Model representing the discriminator. \n",
    "        The model takes two images as inputs (input and target) and outputs a matrix \n",
    "        of patch classifications, where each value represents whether a specific patch \n",
    "        of the image is real or fake.\n",
    "    \"\"\"\n",
    "    # Weight initialization as described in the original paper\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    input_image  = tf.keras.layers.Input(shape=image_shape, name = 'input_image')\n",
    "    target_image = tf.keras.layers.Input(shape=image_shape, name = 'target_image')    \n",
    "    # Concatenate the input and target image (for conditional GAN)\n",
    "    x = tf.keras.layers.concatenate([input_image, target_image])\n",
    "    # PatchGAN architecture\n",
    "    # C64 - Conv layer with 64 filters, 4x4 kernel, strides of 2, followed by LeakyReLU\n",
    "    down1 = downsample (filters = 64,   kernel_size=4, apply_batchnorm = False) (x)\n",
    "    # C128\n",
    "    down2 = downsample (filters = 128, kernel_size=4, apply_batchnorm = True) (down1)\n",
    "    # C256\n",
    "    down3 = downsample (filters = 256, kernel_size=4, apply_batchnorm = True) (down2)\n",
    "    # C512- \n",
    "    down4 = downsample (filters = 512, kernel_size=4,  apply_batchnorm = True) (down3)\n",
    "    # The shape of down4 is 16x16 in which each pixel represents a 70x70 patch in the input image\n",
    "    output = tf.keras.layers.Conv2D(filters=1, kernel_size=4, strides=1, padding= 'same',kernel_initializer=initializer)(down4)\n",
    "    patch_output = tf.keras.layers.Activation('sigmoid')(output)\n",
    "    discriminator_model = tf.keras.Model(inputs = [input_image, target_image], outputs = patch_output)\n",
    "    return discriminator_model\n",
    "# create a model\n",
    "disc_model = Discriminator()\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits= False) # because we have used sigmoid in the last layer of discriminator\n",
    "def generator_loss(discriminator_generated_output, generative_output,  target):\n",
    "    \"\"\"this function calculates the loss value for the generator\n",
    "\n",
    "    Args:\n",
    "        discriminator_generated_output (tf.Tensor): output of the discriminator for the generated output\n",
    "        generative_output (tf.Tensor): generated output from generator\n",
    "        target (tf.Tensor): ground truth\n",
    "\n",
    "    Returns:\n",
    "        Tensor: it returns three different loss values\n",
    "    \"\"\"\n",
    "    # label of real image = 1   label of fake image = 0\n",
    "    gan_loss = loss_object(tf.ones_like(discriminator_generated_output), discriminator_generated_output)\n",
    "    # L1 is used instead of L2 because L1 encourages less blurring\n",
    "    L1_loss = tf.reduce_mean(tf.abs(target - generative_output))\n",
    "    total_gen_loss = gan_loss + (LAMBDA * L1_loss)\n",
    "    return total_gen_loss, gan_loss, L1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator_real_output, discriminator_generated_output):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        discriminator_real_output (tf.Tensor): Discriminator's output for real images\n",
    "        discriminator_generated_output (tf.Tensor): Discriminator's output for generated/fake images\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Total loss value\n",
    "    \"\"\"\n",
    "    # label of real image = 1   label of fake image = 0\n",
    "    real_loss = loss_object(tf.ones_like(discriminator_real_output), discriminator_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(discriminator_real_output), discriminator_generated_output)\n",
    "    total_disc_loss= real_loss + generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1= 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1= 0.5)\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer, \n",
    "                                                     discriminator_optimizer = discriminator_optimizer,\n",
    "                                                     discriminator = disc_model,\n",
    "                                                     generator = gen_model,\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    \"\"\"This function allows you to generate and visualize the input image, ground truth image, and predicted image using a generator model.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained generator model that takes the input image and generates the predicted image.\n",
    "        test_input (tf.Tensor): A tensor representing the input image to the generator. Shape is expected to be [batch_size, height, width, channels].\n",
    "        tar (tf.Tensor): A tensor representing the target or ground truth image. It should have the same shape as the `test_input` (excluding batch size).\n",
    "    \"\"\"\n",
    "    prediction = model ( test_input, training= True)\n",
    "    plt.figure(figsize= (15,15))\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title= ['Input Image', 'Ground Truth', 'Prediction Image']\n",
    "    for i in range(3):\n",
    "        plt.subplot(1,3, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary writer\n",
    "log_dir = \"logs/\"\n",
    "# Check if the directory exists, and create it if not\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "# this decorator converts the 'train_step' function into a TensorFlow graph function, improving its performance.\n",
    "@tf.function\n",
    "def train_step (input_image, target, step):\n",
    "    with tf.GradientTape() as generative_tape, tf.GradientTape() as discriminator_tape:\n",
    "        # Generate an output\n",
    "        generative_output= gen_model(input_image, training = True)\n",
    "        # Check decision of discriminator in detecting real image\n",
    "        discriminator_real_output = disc_model([input_image, target], training = True)\n",
    "        # Check decision of discriminator in detecting fake/generated image\n",
    "        discriminator_generated_output = disc_model([generative_output, target], training = True)\n",
    "        # Calculate loss values\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(discriminator_generated_output, generative_output, target)\n",
    "        disc_loss = discriminator_loss (discriminator_real_output, discriminator_generated_output)\n",
    "        # Compute the gradients of the generator and discriminator losses\n",
    "        generator_gradients = generative_tape.gradient(gen_total_loss, gen_model.trainable_variables)\n",
    "        discriminator_gradients = discriminator_tape.gradient(disc_loss, disc_model.trainable_variables)\n",
    "        # Apply the computed gradients to update the trainable variables of the generator and discriminator models\n",
    "        generator_optimizer.apply_gradients (zip(generator_gradients, gen_model.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, disc_model.trainable_variables))\n",
    "        # Write the generator and discriminator losses as scalar summaries for visualization using TensorBoard\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('generative_total_loss', gen_total_loss, step= step//1000)\n",
    "            tf.summary.scalar('generative_gan_loss', gen_gan_loss, step= step//1000)\n",
    "            tf.summary.scalar('generative_l1_loss', gen_l1_loss, step= step//1000)\n",
    "            tf.summary.scalar('discriminator_loss', disc_loss,  step= step//1000)\n",
    "def fit(train_dataset, test_dataset, steps):\n",
    "    example_input, example_target = next(iter(test_dataset.take(1)))\n",
    "    start= time.time()\n",
    "    for step, (input_image, target) in train_dataset.repeat().take(steps).enumerate():\n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output(wait= True)\n",
    "            if step !=0:\n",
    "                print(f'Time taken for 1000 steps: {time.time() - start:.02f} sec\\n')\n",
    "            start = time.time()\n",
    "            generate_images(gen_model, example_input, example_target)\n",
    "            print(f\"Step: {step//1000}k\")\n",
    "        train_step(input_image, target, step)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print('.', end= '', flush = True)\n",
    "        if (step +1) % 5000 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "# train the model\n",
    "fit(train_dataset, test_dataset, steps=4000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
